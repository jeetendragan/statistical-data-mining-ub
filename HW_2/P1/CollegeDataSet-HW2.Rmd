---
title: "P1) CollegeDataSet-HW2"
author: "Jeetendra Gan | jgan2 | 50325023"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(ISLR)
library(glmnet)
library(pls)
library(gridExtra)
library(grid)
```

### College data set analysis
The data set has 17 predictors and 777 observations.
```{r}
data("College")
names(College)
```

Following is the summary of each feature
```{r}
summary(College)
```

Only the "Private" feature is categorical otherwise everything else is quantitative. I have changed the "Private" variable to a quantitative value. It will be 1 if the college is private, else it will be 0.

Here is the covariance matrix of each of the variables
```{r}
corrgram::corrgram(College)
```
There is a positive correlation between a few coefficients, which may be eliminated by the techniques used below.

### a) Validation set approach

I have divided the data into 70% training set and 30% test set.
```{r}
data <- na.omit(College)
isPrivate = as.numeric(data$Private) - 1
data$Private <- isPrivate
set.seed(1)
trainingIndices = sample(1:nrow(data), round(nrow(data)*0.7))
trainingData = data[trainingIndices, ]
validationData = data[-trainingIndices, ]
validationSetFit <- lm(Apps ~ ., data = trainingData)
```
Here is the summary of the linear fit on the training data. 
```{r}
summary(validationSetFit)
```

The R-Squared value is 0.929, which indicates that the amount of variance explained by the model, is pretty decent. It may also be an indicator that the model does not overfit the data as R-squared is not too large. Important variables indicated by the model are 


1. **PrivateYes ** 
2. **Accept **
3. **Enroll **
4. **Top10perc **
5. Top25perc
6. **Outstate **
7. Room.Board
8. **Expend **
9. Grad.Rate

The highlighted features have smaller p values indicating their importance in the model.

```{r}
trainPred <- predict(validationSetFit, trainingData)
err <- trainPred - trainingData$Apps
trainRSS = sum((err)^2)
trainRMSE = sqrt(trainRSS/nrow(trainingData))
```

The train RSS is:
```{r}
round(trainRSS,digits = 2)
```
The train RMSE is:
```{r}
trainRmseVS = round(trainRMSE, digits = 2)
trainRmseVS
```

**Test error**

The test RSS is 
```{r}
pred <- predict(validationSetFit, validationData)
err <- pred - validationData$Apps
testRSS = sum((err)^2)
round(testRSS, digits = 2)
```

The test RMSE is 
```{r}
testRMSE = sqrt(testRSS / nrow(validationData))
testRmseVS = round(testRMSE, digits = 2)
testRmseVS
```

The train root mean square error is slghtly more than test root mean square error. This indicates that the model generalizes well.

### b) Ridge regression

The Mean Squared error for Ridge regression is as shown below. The error increases with increase in penalization term due to addition of bias.
```{r}
ridge.mod = glmnet(as.matrix(data[, c(-2)]), data[, c(2)], alpha = 0)
X = as.matrix(trainingData[, -c(2)])
Y = trainingData[, 2]
cvRidge = cv.glmnet(X, Y, alpha=0)
plot(cvRidge)
```
The best lambda is:
```{r}
bestLam <- cvRidge$lambda.min
round(bestLam, digits = 2)
```

The coefficients associated with the lambda are as follows:
```{r}
ridge.pred <- predict(ridge.mod, s = bestLam, type="coefficients")
ridge.pred
```

```{r}
testMatrix = as.matrix(validationData[, -2])
ridge.pred2 <- predict(ridge.mod, s = bestLam, type="response", newx = testMatrix)
y_pred <- ridge.pred2
ridgeRSS = sum((y_pred - validationData[, 2])^2)
ridgeRSS
ridgeRMSE = sqrt(ridgeRSS / nrow(validationData))
```
The test RMSE for ridge regression is:
```{r}
round(ridgeRMSE, digits = 2)
```

### c) The LASSO

Here is a plot that shows the elimination of coefficients in the LASSO.
```{r}
lasso.mod <- glmnet(as.matrix(trainingData[, -2]), trainingData[, 2], alpha = 1)
plot(lasso.mod)
```
Graph below shows how the model performs(MSE) on the training data with increase in lambda. 
```{r}
cv.out = cv.glmnet(as.matrix(trainingData[, -2]), trainingData[, 2], alpha = 1)
plot(cv.out)
```
The best lambda for lasso is:
```{r}
lassoBestLam = cv.out$lambda.min
round(lassoBestLam, digits = 2)
```
The coefficients for the best lambda are:
```{r}
predict <- predict(lasso.mod, s = lassoBestLam, type = "coefficients")
predict
```
The lasso model with the best lambda has 12 predictors.
```{r}
lassoPredictTest <- predict(lasso.mod, s = lassoBestLam, type = "response", newx = as.matrix(validationData[, -2]))
lassoRss <- sum((lassoPredictTest - validationData[, 2])^2)
lassoRMSE <- sqrt(lassoRss / nrow(validationData))
```
Lasso RMSE for test data is-
```{r}
lassoRMSE
```

### e) Principle component analysis

Here are the principle components.
```{r}
set.seed(2)
pcr.fit = pcr(Apps ~., data = trainingData, scale = TRUE, validation = "none")
summary(pcr.fit)
```
The graph below shows the Root RMSE (both train and test) for model with each subset of principle components.
```{r}
# Evaluate performance of the model with "i" components in the pca regression for test and training.
pcrTrainRMSE <- c()
pcrTestRMSE <- c()
for (i in 1:17){
	pcr.pred.train = predict(pcr.fit, trainingData, ncomp = i)
	pcr.pred.test = predict(pcr.fit, validationData, ncomp = i)
	train.error <- sqrt(mean((pcr.pred.train - trainingData[, 2])^2))
	test.error <- sqrt(mean((pcr.pred.test - validationData[, 2])^2))
	pcrTrainRMSE <- c(pcrTrainRMSE, train.error)
	pcrTestRMSE <- c(pcrTestRMSE, test.error)
}
plot(pcrTrainRMSE, col = "blue", xlab="Principle Components", ylab="Root MSE", pch=19, type="b",ylim=range(1000, 4500))
points(pcrTestRMSE, pch = 19, type = "b")
legend("topright", legend = c("Training", "Validation"), col=c("blue","black"), pch=19)
```

The model performs better on the test data set. The lowest error observed is for the the model with 17 components. But it is still comparable to a model with 9 principle components. It is better to choose a model with 9 principle components to allow model to generalize better.

***Using cross validation to select the best PCs***
The plot below shows MESP as a function of number of principle components.
```{r}
pcr.fitCv = pcr(Apps ~. , data = trainingData, scale = TRUE, validation = "CV")
validationplot(pcr.fitCv, val.type = "MSEP")
```
As seen in the graph, the fit gets better as we add more components. The graph is simular to the one without cross validation except that both are on a different scale, but their trajectory is the same. In this case too, a model with more than 9 PCs is not performing too better than the one with 9 PCs.

The test RMSE obtained using 9 components is:

```{r}
testRmsePcr = round(pcrTestRMSE[9], digits=2)
testRmsePcr
```

### f) Partial Least Squares
Here are all the least square coefficients
```{r}
set.seed(1)
pls.fit = plsr(Apps ~., data = trainingData, scale = TRUE, validation = "none")
summary(pls.fit)
```

Graph below shows the test and train RMSE for each subset of the principle components.
```{r}
plsTrainRMSE <- c()
plsTestRMSE <- c()
for (i in 1:17){
	pls.pred.train = predict(pls.fit, trainingData, ncomp = i)
	pls.pred.test = predict(pls.fit, validationData, ncomp = i)
	train.error <- sqrt(mean((pls.pred.train - trainingData[, 2])^2))
	test.error <- sqrt(mean((pls.pred.test - validationData[, 2])^2))
	plsTrainRMSE <- c(plsTrainRMSE, train.error)
	plsTestRMSE <- c(plsTestRMSE, test.error)
}
plot(plsTrainRMSE, col = "blue", xlab="Principle Components", ylab="Root MSE", pch=19, type="b",ylim=range(1000, 2000))
points(plsTestRMSE, pch = 19, type = "b")
legend("topright", legend = c("Training", "Validation"), col=c("blue","black"), pch=19)
```
The minimum RMSE obtained for the training data set is 1000.23, which is for the model with 15 components. The minimum does not do much better than the model with 6 principle components. The RMSE for the model with 6 PC is 1019.06. So, for PLS, these 6 principle components will be selected in the final model.

***Selecting the number of principle components with cross validation***
```{r}
pls.fitCv = plsr(Apps ~., data = trainingData, scale = TRUE, validation = "CV")
summary(pls.fitCv)
validationplot(pls.fitCv, val.type = "MSEP")
```
Again the tragectory for the PLS fit with and without cross validation is the same. The fit starts getting better after 5, at around 6. In the non-cross validation approach the value of 6 was selected as the best. After cross validation, the value of 6 seems to be the best as adding more components is not showing much improvement.

The test RMSE obtained using PLS with 6 components is:
```{r}
testRmsePls = round(plsTrainRMSE[6], digits = 2)
testRmsePls
```


### Commment on the results
Following table shows the RMSEs obtained for each of the models.
```{r}
ModelNames = c("LS", "Ridge","Lasso", "PCR", "PLS")
TestErrors = c(testRmseVS, ridgeRMSE, lassoRMSE, testRmsePcr, testRmsePls)
TestErrorComplete = data.frame(ModelNames, TestErrors)
grid.table(TestErrorComplete)
```

Out of all the models, ridge regression performs slightly better. 


**Is there much difference between the test erros obtained?**


There is not much difference in the errors obtained. They all lie closely. 


**How accurately can we predict the number of college applications received?**


If we select the best model of all, the Ridge it produces an error of 994. i.e it may underestimate or overestimate the number of applications received to a university. We need to figure out how significant this estimation can be in terms of the current data that we have. 


Below is a histogram of the number of college applications received in the complete data. Most of the values are clustered below 10000. 
```{r}
hist(data$Apps)
```

Below is a box plot for the number of applications.
```{r}
boxplot(data$Apps)
summary(data$Apps)
```

The data is very closly squished below < ~4000. Which amounts for around 75% of the data. If we assume our best, the ridge, our estimate is going to be around **25%** off from the actual value. Which may not be large for 4000, but increases as we go towards smaller value. 


At 50 percentile, out estimate is going to be off by around 30%. Which again might not be too much. 

In general, out model is off by around 1000 applications all the time. For universities receiving larger applications, it may be a very small value, but for universities with less number of applications, the error is large.

We could be off on both the sides. Consider an example when we have 2000 applications, the model may estimate 1000 or may even estimate 3000. The value range is 2000, which is as large as the true number of applications received!

I would say that the model is decently accurate. Especially if we are dealing with an college application predictions where lesser accuracy might be acceptabe.
